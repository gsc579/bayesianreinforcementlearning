\documentclass[11pt, twoside]{article}

\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{enumerate}
\usepackage{titleps}
\usepackage[top=1.25in,bottom=1in,right=1in,left=1in]{geometry}
\usepackage[parfill]{parskip}
\usepackage{titling}
\usepackage[authoryear]{natbib}

\newpagestyle{ruled}
{\setfoot{}{\thepage}{} \footrule}
\pagestyle{ruled}


\setlength{\droptitle}{-4em}   % This is your set screw
\posttitle{\par\end{center}\vskip 0.5em}


\title{6.882 Proposal: Bayesian Reinforcement Learning}
\date{}
\author {Vickie Ye and Alexandr Wang}


\begin{document}
\maketitle

\section{Background and Motivation}
Because of the recent excitement regarding DeepMind's AlphaGo AI, we wanted to
explore reinforcement learning, in a Bayesian framework. We will use Strens's
ICML-2000 paper, \textit{A Bayesian Framework for Reinforcement Learning} and Engel's ICML-2005 paper, \textit{Reinforcement learning with Gaussian processes} as
guidelines as to how to reason about the reinforcement learning problems with Bayesian formulations.

We will study a toy reinforcement learning problem of navigating an unknown maze.
In the reinforcement learning problem, the agent must explore the surroundings
and determine the behavior that maximizes the expected return. In this case, our
agent must discover the structure of the maze while minimizing the time required
to find the end. A Markov decision process (MDP) model is traditionally used to
model the interactive system over $S$ the set of states, $A$ the set of actions,
$R: S \times A \rightarrow \mathbb{R}$ the reward function, and $T$ the transition
probabilities defined as
\begin{equation}
T(s, a, s') = P(X_{t+1} = s' | X_t = s, Y_t = a).
\end{equation}
We also define a quality function $Q$ as

\begin{equation}
Q(s, a) = \mathbb{E}[R(s, a)] + \gamma \sum_{s'} \textrm{max}_{a'} Q(s', a')
\end{equation}

where $\gamma$ is the time-discount factor \citep[p.3]{strens}.

Our optimal behavior is then the policy that maximizes the quality function. Many
approaches for finding the best policy uses dynamic programming in order to estimate
the transition probabilities. The maximum likelihood estimate for $T(s, a, s')$ is
the proportion of times the action $a$ in state $s$ leads to $s'$, and the estimate of
$\mathbb{E}[R(s, a)]$ is the average reward received whenever $(s, a)$ is taken. This
requires the keeping track of all pairs of states and actions - storage of large
sparse data. In \cite{strens}, the author places a prior over and estimates the
posterior transition probabilities. At each time step, the author uses the current
model over the system to estimate the quality function $Q^*$ and choose the best
action. 

We will also use \cite{engel} as a guideline for implementing Bayesian policy evaluation using Gaussian processes. The algorithm presents a Bayesian solution to policy evaluation termed Gaussian Process Temporal Difference (GPTD). It then extends the algorithm to a State-Action-Reward-State-Action (SARSA) based extension of GPTD termed GPSARSA which is a fully functioning agent that can select actions and gradually improve its policies.

\section{Plans and Consideration}

For the two of us, our plan to implement these models will be in the following tasks:

\begin{enumerate}
\item Implement Bayesian MDPs as described in \cite{strens}. This includes implementing the representation of the posterior, generating the prior, and posterior updates (Vickie).
\item Implement hypothesis generation using the posteriors over the MDP parameters as described in \cite{strens} (Vickie).
\item Replicate paper results in three applications discussed in the paper (``Chain", ``Loop", and ``Maze" toy problems) (Alex).
\item Apply implementation to our maze problem and compare to non-Bayesian methods (in particular, traditional DPs) (Vickie).
\item Implement on-line Monte Carlo GPTD algorithm as described in \cite{engel} (Alex and Vickie).
\item Implement full GP State-Action-Reward-State-Action algorithm (GPSARSA) described in \cite{engel}, an extension of the GPTD which implements a fully learning agent (Alex).
\item Apply implementation of GPSARSA to our maze problem and compare to our Bayesian DP method and non-Bayesian methods from before (Alex).
\end{enumerate}


The major risks of this project come from successfully implementing the GP algorithms \cite{engel}. The algorithm is relatively complicated and involves learning of many parameters, and therefore is very prone to bugs. In general, it seems difficult to test the performance of our algorithms modularly without testing the performance of our agent as a whole. Since our reinforcement learning methods involve multiple techniques working together, we expect challenges in clean, efficient implementations that we can properly test on our toy problems. We anticipate that this proposal contains more work than we will be able to complete, and plan to reevaluate our goals at the halfway point in the project.

\bibliographystyle{apalike}
\bibliography{proposal}

\end{document}
